{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5fa21d44"},"outputs":[],"source":["# Copyright (c) Meta Platforms, Inc. and affiliates."],"id":"5fa21d44"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22246,"status":"ok","timestamp":1689733833505,"user":{"displayName":"김웅기","userId":"06594704263944580607"},"user_tz":-540},"id":"-1LRRHFaxSpx","outputId":"8d74a3d6-a199-4f6f-9f34-bd351f32df46"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"-1LRRHFaxSpx"},{"cell_type":"markdown","metadata":{"id":"b7c0041e"},"source":["# Automatically generating object masks with SAM"],"id":"b7c0041e"},{"cell_type":"markdown","metadata":{"id":"289bb0b4"},"source":["Since SAM can efficiently process prompts, masks for the entire image can be generated by sampling a large number of prompts over an image. This method was used to generate the dataset SA-1B.\n","\n","The class `SamAutomaticMaskGenerator` implements this capability. It works by sampling single-point input prompts in a grid over the image, from each of which SAM can predict multiple masks. Then, masks are filtered for quality and deduplicated using non-maximal suppression. Additional options allow for further improvement of mask quality and quantity, such as running prediction on multiple crops of the image or postprocessing masks to remove small disconnected regions and holes."],"id":"289bb0b4"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7520,"status":"ok","timestamp":1689733841022,"user":{"displayName":"김웅기","userId":"06594704263944580607"},"user_tz":-540},"id":"3VdDm1DTzL0q","outputId":"4ad1afe8-ecf9-4fa6-db62-bb8b914bc524"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/facebookresearch/segment-anything.git\n","  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-g6hwq_m5\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-g6hwq_m5\n","  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: segment-anything\n","  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36589 sha256=5f1272eac42178eb9c227ea294eec8596c18f78ffe7c64ad0f151b1c20ce7760\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ybryv0ew/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n","Successfully built segment-anything\n","Installing collected packages: segment-anything\n","Successfully installed segment-anything-1.0\n"]}],"source":["pip install git+https://github.com/facebookresearch/segment-anything.git"],"id":"3VdDm1DTzL0q"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7117,"status":"ok","timestamp":1689733848128,"user":{"displayName":"김웅기","userId":"06594704263944580607"},"user_tz":-540},"id":"QXLK2ecQzNfN","outputId":"ddfb4bd0-c769-4a61-b38a-4b9062deacd8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Collecting onnxruntime\n","  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnx\n","  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.41.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Collecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.11.1)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Installing collected packages: onnx, humanfriendly, coloredlogs, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.14.0 onnxruntime-1.15.1\n"]}],"source":["pip install opencv-python pycocotools matplotlib onnxruntime onnx"],"id":"QXLK2ecQzNfN"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":42},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1689733848129,"user":{"displayName":"김웅기","userId":"06594704263944580607"},"user_tz":-540},"id":"072e25b8","outputId":"77ba4f45-df00-46a7-d2bb-6331487f9147"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n"]},"metadata":{}}],"source":["from IPython.display import display, HTML\n","display(HTML(\n","\"\"\"\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\"\"\"\n","))"],"id":"072e25b8"},{"cell_type":"markdown","metadata":{"id":"c0b71431"},"source":["## Environment Set-up"],"id":"c0b71431"},{"cell_type":"markdown","metadata":{"id":"47e5a78f"},"source":["If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."],"id":"47e5a78f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4fe300fb"},"outputs":[],"source":["using_colab = True"],"id":"4fe300fb"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66900,"status":"ok","timestamp":1689733915023,"user":{"displayName":"김웅기","userId":"06594704263944580607"},"user_tz":-540},"id":"0685a2f5","outputId":"3a4ce689-cf0b-4191-bac8-b54e5878696e"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.0.1+cu118\n","Torchvision version: 0.15.2+cu118\n","CUDA is available: True\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.41.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Collecting git+https://github.com/facebookresearch/segment-anything.git\n","  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-yo0y5o18\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-yo0y5o18\n","  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","--2023-07-19 02:31:02--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 99846 (98K) [image/jpeg]\n","Saving to: ‘images/dog.jpg’\n","\n","dog.jpg             100%[===================>]  97.51K  --.-KB/s    in 0.02s   \n","\n","2023-07-19 02:31:02 (5.00 MB/s) - ‘images/dog.jpg’ saved [99846/99846]\n","\n","--2023-07-19 02:31:02--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.8.248.107, 65.8.248.127, 65.8.248.22, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.8.248.107|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2564550879 (2.4G) [binary/octet-stream]\n","Saving to: ‘sam_vit_h_4b8939.pth’\n","\n","sam_vit_h_4b8939.pt 100%[===================>]   2.39G  89.6MB/s    in 52s     \n","\n","2023-07-19 02:31:54 (47.1 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n","\n"]}],"source":["if using_colab:\n","    import torch\n","    import torchvision\n","    print(\"PyTorch version:\", torch.__version__)\n","    print(\"Torchvision version:\", torchvision.__version__)\n","    print(\"CUDA is available:\", torch.cuda.is_available())\n","    import sys\n","    !{sys.executable} -m pip install opencv-python matplotlib\n","    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n","\n","    !mkdir images\n","    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n","\n","    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"],"id":"0685a2f5"},{"cell_type":"markdown","metadata":{"id":"fd2bc687"},"source":["## Set-up"],"id":"fd2bc687"},{"cell_type":"code","execution_count":null,"metadata":{"id":"560725a2"},"outputs":[],"source":["import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","import cv2\n","import matplotlib.patches as patches\n","import matplotlib.colors as mcolors"],"id":"560725a2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"74b6e5f0"},"outputs":[],"source":["def show_anns(anns):\n","    if len(anns) == 0:\n","        return\n","    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n","    ax = plt.gca()\n","    ax.set_autoscale_on(False)\n","\n","    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n","    img[:,:,3] = 0\n","    for ann in sorted_anns:\n","        m = ann['segmentation']\n","        color_mask = np.concatenate([np.random.random(3), [0.35]])\n","        img[m] = color_mask\n","    ax.imshow(img)"],"id":"74b6e5f0"},{"cell_type":"markdown","metadata":{"id":"b8c2824a"},"source":["## Automatic mask generation"],"id":"b8c2824a"},{"cell_type":"markdown","metadata":{"id":"d9ef74c5"},"source":["To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended."],"id":"d9ef74c5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"33XDChJ10GI2"},"outputs":[],"source":["from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n","sam = sam_model_registry[\"default\"](checkpoint=\"/content/sam_vit_h_4b8939.pth\")\n","mask_generator = SamAutomaticMaskGenerator(sam)"],"id":"33XDChJ10GI2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1848a108"},"outputs":[],"source":["import sys\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","sam_checkpoint = \"/content/sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)"],"id":"1848a108"},{"cell_type":"markdown","metadata":{"id":"d6b1ea21"},"source":["To generate masks, just run `generate` on an image."],"id":"d6b1ea21"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":670,"status":"ok","timestamp":1689147334671,"user":{"displayName":"김웅기","userId":"06594704263944580607"},"user_tz":-540},"id":"9OZZSDgIBs_P","outputId":"6b1ed09c-19b6-41cf-f396-51707769904c"},"outputs":[{"data":{"text/plain":["Sam(\n","  (image_encoder): ImageEncoderViT(\n","    (patch_embed): PatchEmbed(\n","      (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n","    )\n","    (blocks): ModuleList(\n","      (0-31): 32 x Block(\n","        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n","          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n","          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n","          (act): GELU(approximate='none')\n","        )\n","      )\n","    )\n","    (neck): Sequential(\n","      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (1): LayerNorm2d()\n","      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (3): LayerNorm2d()\n","    )\n","  )\n","  (prompt_encoder): PromptEncoder(\n","    (pe_layer): PositionEmbeddingRandom()\n","    (point_embeddings): ModuleList(\n","      (0-3): 4 x Embedding(1, 256)\n","    )\n","    (not_a_point_embed): Embedding(1, 256)\n","    (mask_downscaling): Sequential(\n","      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n","      (1): LayerNorm2d()\n","      (2): GELU(approximate='none')\n","      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n","      (4): LayerNorm2d()\n","      (5): GELU(approximate='none')\n","      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (no_mask_embed): Embedding(1, 256)\n","  )\n","  (mask_decoder): MaskDecoder(\n","    (transformer): TwoWayTransformer(\n","      (layers): ModuleList(\n","        (0-1): 2 x TwoWayAttentionBlock(\n","          (self_attn): Attention(\n","            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (cross_attn_token_to_image): Attention(\n","            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n","            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n","            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n","            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n","          )\n","          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLPBlock(\n","            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n","            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n","            (act): ReLU()\n","          )\n","          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (cross_attn_image_to_token): Attention(\n","            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n","            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n","            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n","            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n","          )\n","        )\n","      )\n","      (final_attn_token_to_image): Attention(\n","        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n","        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n","        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n","        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n","      )\n","      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (iou_token): Embedding(1, 256)\n","    (mask_tokens): Embedding(4, 256)\n","    (output_upscaling): Sequential(\n","      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n","      (1): LayerNorm2d()\n","      (2): GELU(approximate='none')\n","      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n","      (4): GELU(approximate='none')\n","    )\n","    (output_hypernetworks_mlps): ModuleList(\n","      (0-3): 4 x MLP(\n","        (layers): ModuleList(\n","          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n","          (2): Linear(in_features=256, out_features=32, bias=True)\n","        )\n","      )\n","    )\n","    (iou_prediction_head): MLP(\n","      (layers): ModuleList(\n","        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n","        (2): Linear(in_features=256, out_features=4, bias=True)\n","      )\n","    )\n","  )\n",")"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["sam"],"id":"9OZZSDgIBs_P"},{"cell_type":"markdown","metadata":{"id":"e36a1a39"},"source":["Mask generation returns a list over masks, where each mask is a dictionary containing various data about the mask. These keys are:\n","* `segmentation` : the mask\n","* `area` : the area of the mask in pixels\n","* `bbox` : the boundary box of the mask in XYWH format\n","* `predicted_iou` : the model's own prediction for the quality of the mask\n","* `point_coords` : the sampled input point that generated this mask\n","* `stability_score` : an additional measure of mask quality\n","* `crop_box` : the crop of the image used to generate this mask in XYWH format"],"id":"e36a1a39"},{"cell_type":"markdown","metadata":{"id":"53009a1f"},"source":["Show all the masks overlayed on the image."],"id":"53009a1f"},{"cell_type":"markdown","metadata":{"id":"00b3d6b2"},"source":["## Automatic mask generation options"],"id":"00b3d6b2"},{"cell_type":"markdown","metadata":{"id":"183de84e"},"source":["There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:"],"id":"183de84e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"68364513"},"outputs":[],"source":["mask_generator_2 = SamAutomaticMaskGenerator(\n","    model=sam,\n","    points_per_side=25,\n","    pred_iou_thresh=0.86,\n","    stability_score_thresh=0.92,\n","    crop_n_layers=1,\n","    crop_n_points_downscale_factor=2,\n","    min_mask_region_area=100,  # Requires open-cv to run post-processing\n",")"],"id":"68364513"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1689733930314,"user":{"displayName":"김웅기","userId":"06594704263944580607"},"user_tz":-540},"id":"pMVDdUeyC90_","outputId":"1b11b0f9-bcb2-4e22-9bfd-5fc6163038fe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["22"]},"metadata":{},"execution_count":12}],"source":["from glob import glob\n","\n","img_list = glob('/content/drive/MyDrive/seg_any/*.jpg')\n","len(img_list)"],"id":"pMVDdUeyC90_"},{"cell_type":"code","source":["img_list"],"metadata":{"id":"tlmSTYwKPh7_"},"id":"tlmSTYwKPh7_","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1689733930315,"user":{"displayName":"김웅기","userId":"06594704263944580607"},"user_tz":-540},"id":"CtOZpzG7H_QA","outputId":"c8f56ac9-9b6c-47b8-c1ba-0b56a480d173"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'20230620_100500_부식'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["img_list[0][31:][:-4]"],"id":"CtOZpzG7H_QA"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqc_0P1yDgZx"},"outputs":[],"source":["for i in range(0,22):\n","    names = img_list[i]\n","    name = names[31:][:-4]\n","    img_l = cv2.imread(img_list[i])\n","    img_l = cv2.cvtColor(img_l, cv2.COLOR_BGR2RGB)\n","    masks2 = mask_generator_2.generate(img_l)\n","    plt.figure(figsize=(20,20))\n","    plt.imshow(img_l)\n","    show_anns(masks2)\n","    plt.axis('off')\n","    plt.savefig( fname = '/content/drive/MyDrive/SAM_25/{0}.jpg'.format(name)  , format = 'jpg' )\n","    plt.show()"],"id":"dqc_0P1yDgZx"},{"cell_type":"code","source":["mask_generator_2 = SamAutomaticMaskGenerator(\n","    model=sam,\n","    points_per_side=20,\n","    pred_iou_thresh=0.86,\n","    stability_score_thresh=0.92,\n","    crop_n_layers=1,\n","    crop_n_points_downscale_factor=2,\n","    min_mask_region_area=100,  # Requires open-cv to run post-processing\n",")"],"metadata":{"id":"PlyiWlWkw3U8"},"id":"PlyiWlWkw3U8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(0,22):\n","    names = img_list[i]\n","    name = names[31:][:-4]\n","    img_l = cv2.imread(img_list[i])\n","    img_l = cv2.cvtColor(img_l, cv2.COLOR_BGR2RGB)\n","    masks2 = mask_generator_2.generate(img_l)\n","    plt.figure(figsize=(20,20))\n","    plt.imshow(img_l)\n","    show_anns(masks2)\n","    plt.axis('off')\n","    plt.savefig( fname = '/content/drive/MyDrive/SAM_20/{0}.jpg'.format(name)  , format = 'jpg' )\n","    plt.show()"],"metadata":{"id":"vR6ikbHrw1Nb"},"id":"vR6ikbHrw1Nb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#color 지정 함수\n","def get_color(index):\n","    colors=['b','g','r','c','m','y']\n","    # 인덱스가 색상 목록의 범위를 벗어나면 첫 번째 색상을 반환합니다.\n","    if index < 0 or index >= len(colors):\n","        return colors[index % len(colors)]\n","    return colors[index]"],"metadata":{"id":"Le5arpdMhrDa"},"id":"Le5arpdMhrDa","execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(0,22):\n","    names = img_list[i]\n","    name = names[31:][:-4]\n","    img_l = cv2.imread(img_list[i])\n","    img_l = cv2.cvtColor(img_l, cv2.COLOR_BGR2RGB)\n","    masks2 = mask_generator_2.generate(img_l)\n","    plt.figure(figsize=(20,20))\n","    plt.imshow(img_l)\n","    show_anns(masks2)\n","    plt.axis('off')\n","    plt.savefig( fname = '/content/drive/MyDrive/SAM_25/{0}.jpg'.format(name)  , format = 'jpg' )\n","    plt.show()\n","    for z in range(1):\n","        fig_size=(10,10)\n","        plt.figure(figsize=fig_size)\n","        plt.imshow(img_l)\n","        for x in range(len(masks2)):\n","            box=masks2[x]['bbox']\n","            shp=patches.Rectangle((box[0],box[1]), box[2],box[3], fill=False,edgecolor=get_color(x),\n","                                linewidth=1.5)\n","            plt.gca().add_patch(shp)\n","            plt.text(box[0]+fig_size[0]/5*1,box[1]-fig_size[1]/4*1,str(x),fontdict={'size':fig_size[0]/4*3,'color':'white'},bbox={'boxstyle':'square','color':get_color(x)})\n","        plt.axis('off')\n","        plt.figure(figsize=(10,10))\n","        plt.savefig( fname = '/content/drive/MyDrive/bound_25/{0}.jpg'.format(name)  , format = 'jpg' )\n","        plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1JhsWYzB9UCl6FcLKHohkj5UwG9yofr7l"},"id":"auyRnkDSghzR","executionInfo":{"status":"error","timestamp":1689746281052,"user_tz":-540,"elapsed":972350,"user":{"displayName":"김웅기","userId":"06594704263944580607"}},"outputId":"17d6a44c-821f-4be4-cd87-b859be3a5129"},"id":"auyRnkDSghzR","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["np.count(img_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"3hKIle0amNnU","executionInfo":{"status":"error","timestamp":1689744129021,"user_tz":-540,"elapsed":5,"user":{"displayName":"김웅기","userId":"06594704263944580607"}},"outputId":"b0873c89-9f78-412c-8a5a-5e8586e945fa"},"id":"3hKIle0amNnU","execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-a752d368d87d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0m\u001b[1;32m    316\u001b[0m                              \"{!r}\".format(__name__, attr))\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'count'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"RtvJWg-dg2vF"},"id":"RtvJWg-dg2vF","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb","timestamp":1688539032740}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":5}